BERTBasedDistance
==================

Introduction
------------
**BERTBasedDistance** is a distance metric that utilizes the embeddings generated by pre-trained BERT (Bidirectional Encoder Representations from Transformers) models to compute the semantic similarity between two text documents. BERT captures context-dependent meanings of words, making it highly effective for natural language understanding tasks.

Distance Meaning
----------------
The **BERTBasedDistance** measures the semantic difference between two texts by comparing their BERT-based embeddings. It captures the context in which words are used, allowing for a more nuanced comparison that accounts for the overall meaning of the sentences, rather than just their surface form or word frequency.

Formal Definition
-----------------
Let :math:`D_1` and :math:`D_2` be two documents, and :math:`\mathbf{B}(D)` represent the BERT embedding of a document :math:`D`. The **BERTBasedDistance** between the two documents can be computed as:

.. math::
    \text{Distance}(D_1, D_2) = d(\mathbf{B}(D_1), \mathbf{B}(D_2))

Where:
- :math:`\mathbf{B}(D_1)` and :math:`\mathbf{B}(D_2)` are the BERT embeddings for documents :math:`D_1` and :math:`D_2`.
- :math:`d(\mathbf{B}(D_1), \mathbf{B}(D_2))` is a distance function, typically cosine similarity or Euclidean distance, between the BERT embeddings.

.. code-block:: python

    # Example usage comparing two text files
    str1: str= 'Obama speaks to the media in Illinois'
    str2: str = 'The president greets the press in Chicago'

    similarity_score: Optional[float] = BERTBasedDistance.compute(str1, str2)

    if similarity_score is not None:
        print(f"BERT-based similarity between files: {similarity_score}")
    else:
        print("Could not compute BERT-based similarity.")

Academic Reference
------------------
For more information, refer to:

:footcite:t:`BERTBasedDistance`

.. footbibliography::



Conclusion
----------
The **BERTBasedDistance** is a powerful tool for comparing texts by leveraging the rich contextual representations learned by BERT models. It is highly effective for tasks such as text classification, semantic search, and document clustering, where understanding the meaning of sentences is crucial.
