FastTextDistance
=================

Introduction
------------
**FastTextDistance** is a method used to compare two text documents by leveraging the word embeddings generated by the FastText model. FastText is a powerful word representation method that takes into account both word morphology and subword information, making it particularly effective for languages with complex word forms or out-of-vocabulary words. This method is widely used in natural language processing (NLP) tasks such as text classification, semantic similarity, and machine translation.

Distance Meaning
----------------
The **FastTextDistance** measures the similarity between two documents by computing the distance between their vector representations, which are derived from the FastText word embeddings of the individual words in the documents. By capturing both syntactic and semantic information, FastText allows for a more nuanced comparison of documents, even when they contain rare or morphologically complex words. The distance is typically computed based on the aggregated document vectors, which are derived from the individual word embeddings.

Formal Definition
-----------------
Let :math:`D_1` and :math:`D_2` be two documents composed of words :math:`w_1, w_2, ..., w_n`. Each word is represented by a vector :math:`v_{w_i}` in the FastText embedding space. The document vectors :math:`V_{D_1}` and :math:`V_{D_2}` are computed by averaging the word vectors in each document:

.. math::
   V_{D_1} = \frac{1}{n_1} \sum_{i=1}^{n_1} v_{w_i}, \quad V_{D_2} = \frac{1}{n_2} \sum_{i=1}^{n_2} v_{w_i}

The distance between the two documents can be calculated using a variety of distance metrics, such as cosine similarity or Euclidean distance:

- **Cosine similarity**:
  .. math::
      \text{Cosine}(V_{D_1}, V_{D_2}) = \frac{V_{D_1} \cdot V_{D_2}}{||V_{D_1}|| ||V_{D_2}||}

- **Euclidean distance**:
  .. math::
      \text{Euclidean}(V_{D_1}, V_{D_2}) = \sqrt{\sum_{i=1}^{n} (V_{D_1}[i] - V_{D_2}[i])^2}

.. code-block:: python

   # Exemple d'utilisation
   # Supposons que vous avez un modèle FastText pré-entraîné
   # model = FastText.load("fasttext_model.bin")  # Charger un modèle FastText pré-entraîné

   # Pour l'exemple, nous créons un modèle simple avec un corpus minimal
   sentences = [["the", "cat", "sat", "on", "the", "mat"], ["the", "dog", "sat", "on", "the", "mat"]]
   model = FastText(sentences, vector_size=100, window=5, min_count=1, sg=1)

   text1 = "the cat sat on the mat"
   text2 = "the dog sat on the mat"

   fasttext_distance = FastTextDistance(model)
   distance: float = fasttext_distance.compute(text1, text2)

   print(f"FastText Distance: {distance}")

Academic Reference
------------------
For more information on FastText embeddings, see:


Conclusion
----------
**FastTextDistance** provides a robust way to measure the similarity between two text documents by utilizing rich word embeddings that account for both morphological and semantic information. This method is well-suited for tasks involving noisy or morphologically complex text data, making it an important tool in NLP.
